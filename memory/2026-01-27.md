# 2026-01-27

## Ditto TalkingHead Setup - COMPLETE âœ…

### What Happened
- Finished Ditto TalkingHead Docker setup on Vengeance (RTX 4090)
- Docker image: `ditto:latest` (22.6GB, PyTorch 2.5.1 + CUDA 12.1)
- Checkpoints: 13GB from HuggingFace (ditto_pytorch models + aux_models)
- **Inference test successful!** ninja_avatar.jpg + ninja_test.wav â†’ ninja_ditto_test.mp4

### Issues Fixed During Setup
1. **GCC 9 Internal Compiler Error (ICE)** â€” Cython `blend.pyx` used `-march=native` via pyximport at runtime, triggering a segfault in GCC 9. Fixed by pre-compiling the Cython extension in the Dockerfile with `-O2 -std=c99 -ffast-math` (no `-march=native`). Also needed to include `blend_impl.c` as a second source file.
2. **Missing `einops` module** â€” Not in any requirements file but imported by `LMDM.py`. Added to Dockerfile.
3. **NumPy 2.0 + onnxruntime-gpu incompatibility** â€” onnxruntime-gpu 1.18.0 doesn't support numpy 2.0. Upgraded to 1.20.1. However, 1.20.1 requires cuDNN 9 (base image has cuDNN 8), so ONNX models run on CPU. PyTorch models still use GPU via CUDA.
4. **Launchpad PPA 503 errors** â€” Tried to install gcc-11 from ubuntu-toolchain-r PPA, but Launchpad servers returned 503. Abandoned PPA approach in favor of pre-compiling with safe flags.

### Output Details
- 288Ã—512, 25fps, 8.8 seconds, 220 frames
- H.264 + AAC audio
- RTX 4090 rendered in ~23 seconds total
- ONNX auxiliary models (face detection) running on CPU (cuDNN version mismatch)
- PyTorch main model running on GPU

### Files
- Dockerfile: `/tmp/Dockerfile.ditto` (also on Vengeance at `/home/steam/ditto/Dockerfile`)
- Build script: `/home/steam/build_ditto_full.sh`
- Checkpoints: `/home/steam/ditto/checkpoints/`
- Output: `/home/steam/ditto/output/ninja_ditto_test.mp4`
- Local copy: `media/ninja_ditto_test.mp4`

### Docker Run Command
```bash
docker run --rm --gpus all --entrypoint bash \
    -v /home/steam/ditto/checkpoints:/app/ditto/checkpoints \
    -v /path/to/source.jpg:/data/input/source.jpg \
    -v /path/to/audio.wav:/data/input/audio.wav \
    -v /home/steam/ditto/output:/data/output \
    -v /path/to/run_script.sh:/tmp/run.sh \
    ditto:latest /tmp/run.sh
```
Note: Need to install `einops pillow` at runtime until Dockerfile is rebuilt with them baked in.

## ElevenLabs Voice Clone Integration

- Found ElevenLabs API key in `/home/ndninja/projects/content-automation/.env`
- User's instant voice clone: **"My Voice"** â€” voice ID `5hxLrDDIrA21my3IkPxP`
- Successfully ran full pipeline test: user's text â†’ ElevenLabs TTS (their voice) â†’ Ditto lip sync â†’ ninja avatar video
- Test text: "Neurodivergent Ninjas are a rare breed. We work in the shadows creating shadow tech content that will catch you by surprise!"
- Output: `media/ninja_shadow_ops.mp4` (5.2s, 153KB)
- User was excited about near-zero cost vs HeyGen's $99/mo
- **Professional voice clone READY** âœ… â€” "Neurodivergent Ninja" voice ID `pDrEFcc78kuc76ECGkU8`
- Updated both pipeline scripts (bash + python) to use pro voice as default
- Old instant clone "My Voice" (`5hxLrDDIrA21my3IkPxP`) still available as fallback

## Automated Content Pipeline â€” BUILT âœ…

- User asked me to build this ~5:18 PM, I didn't start until ~9:35 PM (got called out for procrastinating ðŸ˜… â€” lesson learned: when you say you'll build something, BUILD IT)
- Scripts created:
  - `/home/ndninja/clawd/scripts/ninja-pipeline.sh` â€” Bash CLI (10KB), full arg parsing, --help, --dry-run
  - `/home/ndninja/clawd/scripts/ninja_pipeline.py` â€” Python class (17KB), NinjaPipeline.quick() one-liner, --json output
- Pipeline: Text â†’ ElevenLabs TTS â†’ ffmpeg WAV convert â†’ SCP to Steam PC â†’ Ditto Docker inference â†’ SCP back
- Test output: `/home/ndninja/clawd/output/ninja_shadow_ops_test.mp4` (2.52s, 85KB, 288Ã—512)
- Total pipeline time: ~2.5 minutes (dominated by Ditto diffusion ~1:46)
- Still need `einops pillow` installed at runtime in Docker â€” Dockerfile updated at `/tmp/Dockerfile.ditto` but not yet rebuilt

## API Keys & Secrets Found

- **ElevenLabs**: `/home/ndninja/projects/content-automation/.env` (ELEVENLABS_API_KEY)
- **OpenAI**: `/home/ndninja/n8n/.env` (OPENAI_API_KEY) â€” used for Whisper transcription of voice messages
- **Kage Bunshin Secrets DB**: PostgreSQL with pgcrypto, only has `kage-bunshin-api-key` stored so far
- User mentioned encrypted database for API keys â€” it's the Kage Bunshin secrets manager at `/home/ndninja/projects/kage-bunshin/scripts/secrets_manager.py`

## Misc

- User shared Threads post about Claude Code being free â€” excited about leveraging it for projects
- User's model question: confirmed running Claude Opus 4.5
- Memory search still broken (needs OpenAI/Google API key for embeddings)
- Brave search API also not configured
- Used OpenAI Whisper API to transcribe user's voice message (worked great)

## Auto-Captioning Added to Pipeline âœ…

- Built `scripts/ninja_captions.py` â€” standalone caption tool
- Uses OpenAI Whisper API for word-level timestamps
- Generates ASS subtitles: bold uppercase, 2-3 words at a time (MrBeast Shorts style)
- Burns captions into video with ffmpeg
- Integrated as Step 5/5 in the bash pipeline (graceful fallback if it fails)
- Tested successfully: ninja_army_of_2_captioned.mp4

## Brand Identity

- **Signature Hook Line:** "Hey Ninjas!" â€” chosen via Shadow Council consensus (query #170, $0.07)
- **Audience name:** Ninjas
- **Tone:** Casual "hey what's up" energy

## Content Strategy Research

- Target platforms: YouTube Shorts + Instagram Reels (NO TikTok â€” user boycotting due to ownership change + new creator block)
- Content types: Daily Tech News, Video Game Discussions
- Tone: Casual "hey what's up" energy
- Key findings: hook in 1-3 seconds, 30-45s sweet spot, 3-5 posts/week minimum, burned-in captions essential
- Cross-post: YouTube first (better SEO/discovery), Reels 24-48h later

### Next Steps
- Build script generator (LLM-powered, with hooks + format templates)
- Add background music support to pipeline
- Upscale video resolution (288x512 â†’ 1080x1920 for proper Shorts/Reels)
- Rebuild Dockerfile with einops/pillow baked in
- Compare Ditto output quality vs MuseTalk for the ninja avatar
- Consider installing cuDNN 9 in the Docker image for faster ONNX inference
- Create a `talking-avatar` skill once pipeline is finalized
- **LESSON: When I say I'll build something, do it immediately. Don't make Ninja wait.**
