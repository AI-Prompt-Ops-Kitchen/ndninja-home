# Memory-Assisted Tool Discovery Implementation Plan

> **For Claude:** REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.

**Goal:** Implement system that surfaces relevant custom tools at session start and during conversation based on context.

**Architecture:** SessionStart/UserPromptSubmit hooks call shared Python script that queries claude-memory database for tool registry, matches against conversation topics/keywords, outputs suggestions in system-reminder format.

**Tech Stack:** Python 3.13, psycopg2, bash hooks, PostgreSQL (claude-memory)

---

## Task 1: Core Tool Discovery Module - Database Layer

**Files:**
- Create: `scripts/tool_discovery/__init__.py`
- Create: `scripts/tool_discovery/database.py`
- Create: `tests/test_tool_discovery_db.py`

**Step 1: Write failing test for database connection**

Create `tests/test_tool_discovery_db.py`:

```python
import pytest
from scripts.tool_discovery.database import ToolDatabase

def test_database_connection():
    """Test database connection to claude-memory"""
    db = ToolDatabase()
    assert db.connect() is True
    db.close()

def test_get_all_tools_empty():
    """Test getting tools when registry is empty"""
    db = ToolDatabase()
    db.connect()
    tools = db.get_all_tools()
    assert isinstance(tools, list)
    db.close()
```

**Step 2: Run test to verify it fails**

```bash
cd /home/ndninja
python3 -m pytest tests/test_tool_discovery_db.py -v
```

Expected: FAIL with "ModuleNotFoundError: No module named 'scripts.tool_discovery'"

**Step 3: Write minimal database implementation**

Create `scripts/tool_discovery/__init__.py`:

```python
"""Tool discovery system for Memory-Assisted Tool Discovery"""
__version__ = "1.0.0"
```

Create `scripts/tool_discovery/database.py`:

```python
import psycopg2
import json
from typing import List, Dict, Optional

class ToolDatabase:
    """Database interface for tool registry in claude-memory"""

    def __init__(self):
        self.conn = None
        self.host = "localhost"
        self.user = "claude_mcp"
        self.database = "claude_memory"

    def connect(self) -> bool:
        """Connect to PostgreSQL database"""
        try:
            self.conn = psycopg2.connect(
                host=self.host,
                user=self.user,
                database=self.database,
                # Password from .pgpass
            )
            return True
        except Exception as e:
            print(f"Database connection failed: {e}")
            return False

    def close(self):
        """Close database connection"""
        if self.conn:
            self.conn.close()

    def get_all_tools(self) -> List[Dict]:
        """Get all tools from registry"""
        if not self.conn:
            return []

        try:
            cursor = self.conn.cursor()
            cursor.execute("""
                SELECT key, content, notes, updated_at
                FROM references
                WHERE category = 'tool'
                ORDER BY updated_at DESC
            """)

            tools = []
            for row in cursor.fetchall():
                key, content, notes, updated_at = row
                tool_data = json.loads(content) if isinstance(content, str) else content
                tools.append({
                    'key': key,
                    'data': tool_data,
                    'notes': notes,
                    'updated_at': updated_at
                })

            cursor.close()
            return tools
        except Exception as e:
            print(f"Error getting tools: {e}")
            return []
```

**Step 4: Run test to verify it passes**

```bash
python3 -m pytest tests/test_tool_discovery_db.py -v
```

Expected: PASS (2/2 tests)

**Step 5: Commit**

```bash
git add scripts/tool_discovery/ tests/test_tool_discovery_db.py
git commit -m "feat: add database layer for tool registry

- ToolDatabase class for PostgreSQL connection
- get_all_tools() method to query tool registry
- Tests for database connection and tool retrieval"
```

---

## Task 2: Core Tool Discovery Module - Scoring Algorithm

**Files:**
- Create: `scripts/tool_discovery/scorer.py`
- Create: `tests/test_tool_scoring.py`

**Step 1: Write failing test for keyword scoring**

Create `tests/test_tool_scoring.py`:

```python
import pytest
from scripts.tool_discovery.scorer import ToolScorer

def test_exact_keyword_match():
    """Test exact keyword match scores +10"""
    scorer = ToolScorer()
    tool = {
        'data': {
            'keywords': ['docker', 'container', 'debug']
        }
    }
    topics = ['docker environment issues']
    score = scorer.score_tool(tool, topics)
    assert score >= 10

def test_partial_keyword_match():
    """Test partial keyword match scores +5"""
    scorer = ToolScorer()
    tool = {
        'data': {
            'keywords': ['multi-model', 'analysis']
        }
    }
    topics = ['need multi-model comparison']
    score = scorer.score_tool(tool, topics)
    assert score >= 5

def test_recently_used_bonus():
    """Test recent usage adds +3 bonus"""
    from datetime import datetime, timedelta
    scorer = ToolScorer()
    recent_date = datetime.now() - timedelta(days=3)
    tool = {
        'data': {
            'keywords': ['test'],
            'last_used': recent_date.strftime('%Y-%m-%d')
        }
    }
    topics = ['test']
    score = scorer.score_tool(tool, topics)
    assert score >= 13  # 10 (exact) + 3 (recent)

def test_high_usage_bonus():
    """Test high use count adds +2 bonus"""
    scorer = ToolScorer()
    tool = {
        'data': {
            'keywords': ['test'],
            'use_count': 15
        }
    }
    topics = ['test']
    score = scorer.score_tool(tool, topics)
    assert score >= 12  # 10 (exact) + 2 (high usage)
```

**Step 2: Run test to verify it fails**

```bash
python3 -m pytest tests/test_tool_scoring.py -v
```

Expected: FAIL with "ModuleNotFoundError: No module named 'scripts.tool_discovery.scorer'"

**Step 3: Write scoring implementation**

Create `scripts/tool_discovery/scorer.py`:

```python
from datetime import datetime, timedelta
from typing import List, Dict

class ToolScorer:
    """Score tools against conversation topics"""

    EXACT_MATCH = 10
    PARTIAL_MATCH = 5
    RECENT_BONUS = 3  # Used in last 7 days
    HIGH_USAGE_BONUS = 2  # Use count > 10

    def score_tool(self, tool: Dict, topics: List[str]) -> int:
        """
        Score a tool against conversation topics

        Args:
            tool: Tool dict with 'data' containing keywords, last_used, use_count
            topics: List of topic strings from conversation

        Returns:
            Total score for this tool
        """
        score = 0
        tool_data = tool.get('data', {})
        keywords = tool_data.get('keywords', [])

        # Tokenize topics
        topic_tokens = []
        for topic in topics:
            topic_tokens.extend(topic.lower().split())

        # Keyword matching
        for keyword in keywords:
            keyword_lower = keyword.lower()

            # Exact match
            if keyword_lower in topic_tokens:
                score += self.EXACT_MATCH
                continue

            # Partial match (substring)
            for token in topic_tokens:
                if keyword_lower in token or token in keyword_lower:
                    score += self.PARTIAL_MATCH
                    break

        # Recent usage bonus
        last_used = tool_data.get('last_used')
        if last_used:
            try:
                last_used_date = datetime.strptime(last_used, '%Y-%m-%d')
                days_ago = (datetime.now() - last_used_date).days
                if days_ago <= 7:
                    score += self.RECENT_BONUS
            except ValueError:
                pass

        # High usage bonus
        use_count = tool_data.get('use_count', 0)
        if use_count > 10:
            score += self.HIGH_USAGE_BONUS

        return score

    def rank_tools(self, tools: List[Dict], topics: List[str], limit: int = 5) -> List[Dict]:
        """
        Rank tools by score and return top N

        Args:
            tools: List of tool dicts
            topics: Conversation topics
            limit: Maximum tools to return

        Returns:
            List of (tool, score) tuples, sorted by score descending
        """
        scored = []
        for tool in tools:
            score = self.score_tool(tool, topics)
            if score > 0:  # Only include tools with matches
                scored.append((tool, score))

        # Sort by score descending
        scored.sort(key=lambda x: x[1], reverse=True)

        return scored[:limit]
```

**Step 4: Run test to verify it passes**

```bash
python3 -m pytest tests/test_tool_scoring.py -v
```

Expected: PASS (4/4 tests)

**Step 5: Commit**

```bash
git add scripts/tool_discovery/scorer.py tests/test_tool_scoring.py
git commit -m "feat: add tool scoring algorithm

- ToolScorer class with keyword matching
- Exact match (+10), partial match (+5) scoring
- Recent usage (+3) and high usage (+2) bonuses
- rank_tools() to get top N tools by score"
```

---

## Task 3: SessionStart Discovery Logic

**Files:**
- Create: `scripts/tool_discovery/session_start.py`
- Create: `tests/test_session_start.py`

**Step 1: Write failing test for topic extraction**

Create `tests/test_session_start.py`:

```python
import pytest
from scripts.tool_discovery.session_start import SessionStartDiscovery

def test_extract_topics_from_conversations():
    """Test extracting topics from conversation summaries"""
    discovery = SessionStartDiscovery()
    conversations = [
        {
            'topics_discussed': ['Docker debugging', 'n8n workflows', 'video production']
        },
        {
            'topics_discussed': ['LLM Council usage', 'VPN analysis']
        }
    ]
    topics = discovery.extract_topics(conversations)
    assert 'Docker debugging' in topics
    assert 'LLM Council usage' in topics
    assert len(topics) >= 5

def test_get_relevant_tools_with_topics():
    """Test getting relevant tools based on topics"""
    discovery = SessionStartDiscovery()
    # This will use real database - create mock if needed
    tools = discovery.get_relevant_tools(limit=3)
    assert isinstance(tools, list)
    assert len(tools) <= 3

def test_format_suggestions():
    """Test formatting tool suggestions for output"""
    discovery = SessionStartDiscovery()
    tools = [
        ({
            'key': 'test-tool',
            'data': {
                'name': 'Test Tool',
                'description': 'A test tool',
                'command': 'test-command'
            }
        }, 15)  # (tool, score)
    ]
    output = discovery.format_suggestions(tools)
    assert 'Test Tool' in output
    assert 'test-command' in output
```

**Step 2: Run test to verify it fails**

```bash
python3 -m pytest tests/test_session_start.py -v
```

Expected: FAIL with "ModuleNotFoundError"

**Step 3: Write SessionStart implementation**

Create `scripts/tool_discovery/session_start.py`:

```python
import sys
import json
from typing import List, Dict, Tuple
from .database import ToolDatabase
from .scorer import ToolScorer

class SessionStartDiscovery:
    """Tool discovery for session startup"""

    def __init__(self):
        self.db = ToolDatabase()
        self.scorer = ToolScorer()

    def extract_topics(self, conversations: List[Dict]) -> List[str]:
        """
        Extract topics from recent conversations

        Args:
            conversations: List of conversation summary dicts

        Returns:
            List of topic strings
        """
        topics = []
        for conv in conversations:
            topics_discussed = conv.get('topics_discussed', [])
            if topics_discussed:
                topics.extend(topics_discussed)
        return topics

    def get_relevant_tools(self, limit: int = 5) -> List[Tuple[Dict, int]]:
        """
        Get relevant tools for session based on recent conversations

        Args:
            limit: Maximum number of tools to return

        Returns:
            List of (tool, score) tuples
        """
        # Get recent conversations from database
        if not self.db.connect():
            return []

        try:
            cursor = self.db.conn.cursor()
            cursor.execute("""
                SELECT topics_discussed
                FROM conversation_summaries
                WHERE app_source = 'code'
                ORDER BY created_at DESC
                LIMIT 3
            """)

            conversations = []
            for row in cursor.fetchall():
                topics = row[0] if row[0] else []
                conversations.append({'topics_discussed': topics})

            cursor.close()
        except Exception as e:
            print(f"Error getting conversations: {e}", file=sys.stderr)
            conversations = []

        # Extract topics
        topics = self.extract_topics(conversations)

        # If no topics, return most used tools
        if not topics:
            all_tools = self.db.get_all_tools()
            # Sort by use_count
            all_tools.sort(key=lambda t: t.get('data', {}).get('use_count', 0), reverse=True)
            self.db.close()
            return [(t, 0) for t in all_tools[:limit]]

        # Get all tools and rank
        all_tools = self.db.get_all_tools()
        self.db.close()

        ranked = self.scorer.rank_tools(all_tools, topics, limit)
        return ranked

    def format_suggestions(self, tools: List[Tuple[Dict, int]]) -> str:
        """
        Format tool suggestions for system-reminder output

        Args:
            tools: List of (tool, score) tuples

        Returns:
            Formatted string for output
        """
        if not tools:
            return "No relevant tools found. Run: python3 ~/scripts/tool_registry_seed.py"

        lines = ["Relevant tools for this session:"]
        for tool, score in tools:
            data = tool.get('data', {})
            name = data.get('name', 'Unknown')
            desc = data.get('description', 'No description')
            cmd = data.get('command', '')

            # Truncate long descriptions
            if len(desc) > 80:
                desc = desc[:77] + "..."

            line = f"  - {name}: {desc}"
            if cmd:
                line += f" ({cmd})"
            lines.append(line)

        return "\n".join(lines)
```

**Step 4: Run test to verify it passes**

```bash
python3 -m pytest tests/test_session_start.py -v
```

Expected: PASS (3/3 tests)

**Step 5: Commit**

```bash
git add scripts/tool_discovery/session_start.py tests/test_session_start.py
git commit -m "feat: add SessionStart discovery logic

- Extract topics from recent conversations
- Rank tools by relevance to topics
- Format suggestions for system-reminder output
- Fallback to most-used tools if no topics"
```

---

## Task 4: Real-time Keyword Detection

**Files:**
- Create: `scripts/tool_discovery/realtime.py`
- Create: `tests/test_realtime.py`

**Step 1: Write failing test for keyword detection**

Create `tests/test_realtime.py`:

```python
import pytest
from scripts.tool_discovery.realtime import RealtimeDiscovery

def test_match_high_value_keywords():
    """Test matching high-value keywords in prompt"""
    discovery = RealtimeDiscovery()
    prompt = "I need to do a multi-model analysis of VPN providers"
    match = discovery.match_keywords(prompt)
    assert match is not None
    assert 'llm-council' in match['key'].lower()

def test_no_match_for_generic_prompt():
    """Test no match for generic prompts"""
    discovery = RealtimeDiscovery()
    prompt = "How do I write a function?"
    match = discovery.match_keywords(prompt)
    assert match is None

def test_rate_limit_check():
    """Test rate limiting logic"""
    discovery = RealtimeDiscovery()
    # First call should allow
    assert discovery.check_rate_limit() is True
    # Immediate second call should block
    assert discovery.check_rate_limit() is False

def test_format_realtime_suggestion():
    """Test formatting real-time suggestion"""
    discovery = RealtimeDiscovery()
    tool = {
        'key': 'llm-council',
        'data': {
            'name': 'LLM Council',
            'description': 'Multi-model analysis',
            'command': 'python3 council.py'
        }
    }
    output = discovery.format_suggestion(tool)
    assert 'LLM Council' in output
    assert 'python3 council.py' in output
```

**Step 2: Run test to verify it fails**

```bash
python3 -m pytest tests/test_realtime.py -v
```

Expected: FAIL with "ModuleNotFoundError"

**Step 3: Write realtime detection implementation**

Create `scripts/tool_discovery/realtime.py`:

```python
import re
import os
import json
import time
from typing import Optional, Dict
from .database import ToolDatabase

class RealtimeDiscovery:
    """Real-time tool discovery based on keywords"""

    # High-value keyword patterns -> tool key
    KEYWORD_PATTERNS = {
        r'multi[- ]?model|consensus|vpn analysis': 'llm-council',
        r'distributed|kage bunshin|72b|cluster': 'kage-bunshin',
        r'docker environment|container debug': 'docker-env-debugger',
    }

    RATE_LIMIT_MESSAGES = 5  # Suggest once per N messages

    def __init__(self):
        self.db = ToolDatabase()
        self.message_count = 0
        self.last_suggestion_count = -999
        self.state_file = f"/tmp/tool-discovery-session-{os.getpid()}.json"
        self._load_state()

    def _load_state(self):
        """Load rate limit state from temp file"""
        try:
            if os.path.exists(self.state_file):
                with open(self.state_file, 'r') as f:
                    state = json.load(f)
                    self.message_count = state.get('message_count', 0)
                    self.last_suggestion_count = state.get('last_suggestion_count', -999)
        except Exception:
            pass

    def _save_state(self):
        """Save rate limit state to temp file"""
        try:
            with open(self.state_file, 'w') as f:
                json.dump({
                    'message_count': self.message_count,
                    'last_suggestion_count': self.last_suggestion_count
                }, f)
        except Exception:
            pass

    def check_rate_limit(self) -> bool:
        """
        Check if suggestion is allowed based on rate limit

        Returns:
            True if suggestion allowed, False if rate-limited
        """
        self.message_count += 1

        # Check if enough messages have passed
        if self.message_count - self.last_suggestion_count < self.RATE_LIMIT_MESSAGES:
            return False

        # Update last suggestion count
        self.last_suggestion_count = self.message_count
        self._save_state()
        return True

    def match_keywords(self, prompt: str) -> Optional[Dict]:
        """
        Match prompt against high-value keyword patterns

        Args:
            prompt: User prompt text

        Returns:
            Tool dict if match found, None otherwise
        """
        prompt_lower = prompt.lower()

        # Check each pattern
        for pattern, tool_key in self.KEYWORD_PATTERNS.items():
            if re.search(pattern, prompt_lower):
                # Get tool from database
                if not self.db.connect():
                    return None

                try:
                    cursor = self.db.conn.cursor()
                    cursor.execute("""
                        SELECT key, content, notes
                        FROM references
                        WHERE category = 'tool' AND key = %s
                    """, (tool_key,))

                    row = cursor.fetchone()
                    cursor.close()
                    self.db.close()

                    if row:
                        key, content, notes = row
                        tool_data = json.loads(content) if isinstance(content, str) else content
                        return {
                            'key': key,
                            'data': tool_data,
                            'notes': notes
                        }
                except Exception as e:
                    self.db.close()
                    return None

        return None

    def format_suggestion(self, tool: Dict) -> str:
        """
        Format real-time tool suggestion

        Args:
            tool: Tool dict

        Returns:
            Formatted suggestion string
        """
        data = tool.get('data', {})
        name = data.get('name', 'Unknown Tool')
        desc = data.get('description', '')
        cmd = data.get('command', '')

        output = f"ðŸ’¡ Relevant tool: {name}"
        if desc:
            output += f" - {desc}"
        if cmd:
            output += f"\n   Usage: {cmd}"

        return output
```

**Step 4: Run test to verify it passes**

```bash
python3 -m pytest tests/test_realtime.py -v
```

Expected: PASS (4/4 tests)

**Step 5: Commit**

```bash
git add scripts/tool_discovery/realtime.py tests/test_realtime.py
git commit -m "feat: add real-time keyword detection

- Regex-based keyword matching for high-value tools
- Rate limiting (1 suggestion per 5 messages)
- State persistence in /tmp for session duration
- Format suggestions with emoji indicator"
```

---

## Task 5: Tool Registry Seed Script

**Files:**
- Create: `scripts/tool_registry_seed.py`

**Step 1: Write seed script (no test needed - operational script)**

Create `scripts/tool_registry_seed.py`:

```python
#!/usr/bin/env python3
"""
Tool Registry Seed Script

Populates the tool registry in claude-memory database with initial high-value tools.
"""

import sys
import json
import psycopg2

TOOLS = [
    {
        'key': 'llm-council',
        'name': 'LLM Council',
        'location': '/home/ndninja/projects/llm-council/',
        'command': 'python3 /home/ndninja/projects/llm-council/council.py "your question"',
        'description': 'Multi-model analysis tool - queries 4 LLMs in parallel (GPT-5.2, Claude Sonnet 4.5, Gemini 3 Pro, Perplexity) and synthesizes results',
        'keywords': ['multi-model', 'research', 'analysis', 'consensus', 'complex-question', 'vpn', 'comparison', 'diverse-perspectives'],
        'usage_examples': [
            'Research: Compare VPN providers with multi-model analysis',
            'Analysis: Evaluate architectural trade-offs from different perspectives',
            'Validation: Cross-check complex technical decisions'
        ],
        'performance': '~3-5 min, $0.15-0.30 per query',
        'last_used': '2026-01-16',
        'use_count': 15
    },
    {
        'key': 'kage-bunshin',
        'name': 'Kage Bunshin',
        'location': '/home/ndninja/projects/kage-bunshin/',
        'command': 'cd /home/ndninja/projects/kage-bunshin && python3 api/main.py',
        'description': 'Distributed AI orchestration system - executes tasks across 4-node cluster (ndnlinuxserv, ndnlinuxsrv2, vengeance, rog-flow-z13) with 72B model capability',
        'keywords': ['distributed', 'kage-bunshin', '72b', 'cluster', 'parallel', 'orchestration', 'multi-node', 'large-model'],
        'usage_examples': [
            'Dispatch: Run complex task across distributed cluster',
            'Parallel: Execute multiple independent tasks simultaneously',
            '72B: Access flagship model for advanced reasoning'
        ],
        'performance': 'Variable - 2-10 min depending on task complexity',
        'last_used': '2026-01-16',
        'use_count': 23
    },
    {
        'key': 'doc-generator',
        'name': 'Documentation Generator',
        'location': '/home/ndninja/scripts/doc_generator/',
        'command': 'python3 /home/ndninja/scripts/doc_generator/main.py --doc-type README --project "Project Name"',
        'description': 'Automated documentation system - generates README, USER_GUIDE, API, MEETING_NOTES, STATUS_REPORT, ONBOARDING docs using Claude API with workspace DB analysis',
        'keywords': ['documentation', 'readme', 'api-docs', 'user-guide', 'status-report', 'onboarding', 'automated-docs'],
        'usage_examples': [
            'README: Generate project README from workspace DB',
            'API: Create API documentation with examples',
            'Status: Weekly status reports via n8n automation'
        ],
        'performance': '~30-60 sec, $0.02-0.09 per doc',
        'last_used': '2026-01-16',
        'use_count': 12
    },
    {
        'key': 'vengeance-validate',
        'name': 'Vengeance Code Validator',
        'location': '/home/ndninja/scripts/',
        'command': 'vengeance-validate <file.py>',
        'description': 'Automated validation pipeline for LLM-generated code - syntax check, linting, formatting, security scan. Supports Python, JavaScript, TypeScript',
        'keywords': ['validation', 'code-quality', 'linting', 'security', 'llm-code', 'vengeance', 'qwen2.5'],
        'usage_examples': [
            'Validate: Check Python script from Vengeance LLM',
            'Security: Scan for vulnerabilities in generated code',
            'Quality: Ensure code meets standards'
        ],
        'performance': '<5 sec per file',
        'last_used': '2026-01-15',
        'use_count': 8
    },
    {
        'key': 'permission-helper',
        'name': 'Permission Helper Plugin',
        'location': '/home/ndninja/.claude/plugins/local/permission-helper/',
        'command': '/plan-permissions "plan text or file path"',
        'description': 'Pre-approves permissions for implementation plans - detects file writes, bash commands, skills, web operations. Updates settings.json automatically',
        'keywords': ['permissions', 'pre-approve', 'plan-analysis', 'settings', 'file-detection', 'bash-detection'],
        'usage_examples': [
            'Pre-approve: Analyze plan and approve all permissions upfront',
            'Rollback: Restore previous permission settings',
            'History: View permission change history'
        ],
        'performance': '<1 sec analysis',
        'last_used': '2026-01-17',
        'use_count': 18
    },
    {
        'key': 'server-health',
        'name': 'Server Health Monitor',
        'location': '/home/ndninja/.claude/plugins/local/server-health/',
        'command': '/health --quick',
        'description': 'Quick server health checks - PostgreSQL, Docker, disk usage, critical services. Full scan with --deep flag',
        'keywords': ['health', 'monitoring', 'postgresql', 'docker', 'disk', 'services', 'diagnostics'],
        'usage_examples': [
            'Quick: Run fast health check (~100ms)',
            'Deep: Full system diagnostics (~2 sec)',
            'Audit: Log results to PostgreSQL'
        ],
        'performance': '100ms quick, 2s deep',
        'last_used': '2026-01-19',
        'use_count': 31
    },
    {
        'key': 'llm-code-validator',
        'name': 'LLM Code Validator',
        'location': '/home/ndninja/scripts/llm-code-validator.py',
        'command': 'python3 /home/ndninja/scripts/llm-code-validator.py <file>',
        'description': 'Generic code validation for any LLM output - syntax, structure, common patterns. Language-agnostic checks',
        'keywords': ['validation', 'llm', 'code-quality', 'syntax', 'generic'],
        'usage_examples': [
            'Validate: Check any code file for basic quality',
            'Syntax: Verify code parses correctly',
            'Patterns: Detect common anti-patterns'
        ],
        'performance': '<3 sec',
        'last_used': '2026-01-14',
        'use_count': 6
    },
    {
        'key': 'reflection-engine',
        'name': 'Skill Reflection Engine',
        'location': '/home/ndninja/scripts/reflection_engine/',
        'command': 'python3 /home/ndninja/scripts/reflection_engine/main.py',
        'description': 'Automated skill improvement system - detects correction signals from conversations, analyzes with LLM Council, updates skill files',
        'keywords': ['reflection', 'skill-improvement', 'automated-learning', 'correction-signals', 'stop-hook'],
        'usage_examples': [
            'Auto-improve: Detect and fix skill issues from conversations',
            'Signals: Analyze correction patterns',
            'Dedup: Prevent duplicate improvements'
        ],
        'performance': '~2-5 min per reflection',
        'last_used': '2026-01-16',
        'use_count': 14
    },
    {
        'key': 'draft-generator',
        'name': 'Content Draft Generator',
        'location': '/home/ndninja/projects/content-automation/',
        'command': 'curl -X POST http://localhost:5002/generate-draft -d \'{"count": 1}\'',
        'description': 'Weekly automated content draft generation - uses LLM Council for research, generates gaming/tech content drafts',
        'keywords': ['content', 'draft', 'automation', 'gaming', 'tech', 'weekly', 'n8n'],
        'usage_examples': [
            'Generate: Create content draft via API',
            'Weekly: Automated Sunday 2:30 AM generation',
            'Research: LLM Council-powered topic research'
        ],
        'performance': '~3 min per draft',
        'last_used': '2026-01-17',
        'use_count': 9
    },
    {
        'key': 'video-production',
        'name': 'Video Production Pipeline',
        'location': '/home/ndninja/projects/content-automation/',
        'command': 'python3 production_orchestrator.py create <draft_id>',
        'description': 'Complete video production workflow - voiceover generation, screen recording, thumbnail creation, assembly, quality validation',
        'keywords': ['video', 'production', 'voiceover', 'thumbnail', 'assembly', 'ffmpeg', 'elevenlabs'],
        'usage_examples': [
            'Create: Start new video production from draft',
            'Status: Check production workflow state',
            'Validate: Quality checks before publishing'
        ],
        'performance': 'Variable - 5-15 min per video',
        'last_used': '2026-01-16',
        'use_count': 7
    }
]

def seed_registry():
    """Seed the tool registry in claude-memory database"""
    try:
        conn = psycopg2.connect(
            host='localhost',
            user='claude_mcp',
            database='claude_memory'
        )
        cursor = conn.cursor()

        print("Seeding tool registry...")
        added = 0
        updated = 0

        for tool in TOOLS:
            key = tool.pop('key')

            # Check if exists
            cursor.execute("""
                SELECT key FROM references
                WHERE category = 'tool' AND key = %s
            """, (key,))

            exists = cursor.fetchone() is not None

            if exists:
                # Update existing
                cursor.execute("""
                    UPDATE references
                    SET content = %s, updated_at = NOW()
                    WHERE category = 'tool' AND key = %s
                """, (json.dumps(tool), key))
                updated += 1
                print(f"  Updated: {tool['name']}")
            else:
                # Insert new
                cursor.execute("""
                    INSERT INTO references (category, key, title, content, context_tag, device, notes, created_at, updated_at)
                    VALUES ('tool', %s, %s, %s, 'always', 'all', 'Memory-Assisted Tool Discovery registry', NOW(), NOW())
                """, (key, tool['name'], json.dumps(tool)))
                added += 1
                print(f"  Added: {tool['name']}")

        conn.commit()
        cursor.close()
        conn.close()

        print(f"\nâœ… Registry seeded successfully!")
        print(f"   Added: {added} tools")
        print(f"   Updated: {updated} tools")
        print(f"   Total: {len(TOOLS)} tools in registry")

        return 0

    except Exception as e:
        print(f"âŒ Error seeding registry: {e}", file=sys.stderr)
        return 1

if __name__ == '__main__':
    sys.exit(seed_registry())
```

**Step 2: Make script executable and test**

```bash
chmod +x scripts/tool_registry_seed.py
python3 scripts/tool_registry_seed.py
```

Expected output:
```
Seeding tool registry...
  Added: LLM Council
  Added: Kage Bunshin
  ...
âœ… Registry seeded successfully!
   Added: 10 tools
```

**Step 3: Verify in database**

```bash
psql -h localhost -U claude_mcp -d claude_memory -c "SELECT key, title FROM references WHERE category='tool' ORDER BY title;"
```

Expected: List of 10 tools

**Step 4: Commit**

```bash
git add scripts/tool_registry_seed.py
git commit -m "feat: add tool registry seed script

Seeds claude-memory database with 10 high-value tools:
- LLM Council, Kage Bunshin, Doc Generator
- Vengeance Validate, Permission Helper
- Server Health, Reflection Engine
- Draft Generator, Video Production
- LLM Code Validator

Updates existing tools, inserts new ones"
```

---

## Task 6: SessionStart Hook

**Files:**
- Create: `.claude/hooks/on-session-start-tool-discovery.sh`

**Step 1: Write SessionStart hook**

Create `.claude/hooks/on-session-start-tool-discovery.sh`:

```bash
#!/bin/bash
#
# SessionStart hook for Memory-Assisted Tool Discovery
# Surfaces relevant tools based on recent conversation topics

SCRIPT_DIR="/home/ndninja/scripts/tool_discovery"
LOG_FILE="/home/ndninja/.logs/tool-discovery.log"

# Create log directory if needed
mkdir -p "$(dirname "$LOG_FILE")"

# Timeout protection (200ms)
timeout 0.2s python3 -c "
import sys
sys.path.insert(0, '/home/ndninja/scripts')

try:
    from tool_discovery.session_start import SessionStartDiscovery

    discovery = SessionStartDiscovery()
    tools = discovery.get_relevant_tools(limit=5)

    if tools:
        output = discovery.format_suggestions(tools)
        print(output)

except Exception as e:
    import traceback
    with open('$LOG_FILE', 'a') as f:
        f.write(f'SessionStart error: {e}\n')
        f.write(traceback.format_exc())
" 2>>"$LOG_FILE"

exit 0  # Never block session start
```

**Step 2: Make hook executable**

```bash
chmod +x .claude/hooks/on-session-start-tool-discovery.sh
```

**Step 3: Test hook manually**

```bash
./.claude/hooks/on-session-start-tool-discovery.sh
```

Expected output:
```
Relevant tools for this session:
  - LLM Council: Multi-model analysis tool - queries 4 LLMs... (python3 /home/ndninja/projects/llm-council/council.py)
  - Kage Bunshin: Distributed AI orchestration system... (cd /home/ndninja/projects/kage-bunshin && python3 api/main.py)
  ...
```

**Step 4: Test in new session**

```bash
# Exit and restart Claude Code
exit
claude
```

Expected: Tool suggestions appear in session startup

**Step 5: Commit**

```bash
git add .claude/hooks/on-session-start-tool-discovery.sh
git commit -m "feat: add SessionStart hook for tool discovery

- Calls SessionStartDiscovery to get relevant tools
- 200ms timeout protection
- Logs errors without blocking session start
- Shows top 5 tools based on recent topics"
```

---

## Task 7: UserPromptSubmit Hook (Real-time)

**Files:**
- Create: `.claude/hooks/on-user-prompt-tool-discovery.sh`

**Step 1: Write UserPromptSubmit hook**

Create `.claude/hooks/on-user-prompt-tool-discovery.sh`:

```bash
#!/bin/bash
#
# UserPromptSubmit hook for Memory-Assisted Tool Discovery
# Real-time keyword detection for high-value tools

SCRIPT_DIR="/home/ndninja/scripts/tool_discovery"
LOG_FILE="/home/ndninja/.logs/tool-discovery.log"

# Get user prompt from stdin or args
PROMPT="${CLAUDE_USER_PROMPT:-$*}"

# Skip if prompt is empty
if [ -z "$PROMPT" ]; then
    exit 0
fi

# Timeout protection (50ms)
timeout 0.05s python3 -c "
import sys
sys.path.insert(0, '/home/ndninja/scripts')

try:
    from tool_discovery.realtime import RealtimeDiscovery

    discovery = RealtimeDiscovery()

    # Check rate limit first
    if not discovery.check_rate_limit():
        sys.exit(0)

    # Match keywords
    tool = discovery.match_keywords('''$PROMPT''')

    if tool:
        output = discovery.format_suggestion(tool)
        print(output)

except Exception as e:
    import traceback
    with open('$LOG_FILE', 'a') as f:
        f.write(f'UserPromptSubmit error: {e}\n')
        f.write(traceback.format_exc())
" 2>>"$LOG_FILE"

exit 0  # Never block prompt submission
```

**Step 2: Make hook executable**

```bash
chmod +x .claude/hooks/on-user-prompt-tool-discovery.sh
```

**Step 3: Test hook manually**

```bash
./.claude/hooks/on-user-prompt-tool-discovery.sh "I need multi-model analysis for VPN comparison"
```

Expected output:
```
ðŸ’¡ Relevant tool: LLM Council - Multi-model analysis tool - queries 4 LLMs in parallel...
   Usage: python3 /home/ndninja/projects/llm-council/council.py "your question"
```

**Step 4: Test rate limiting**

```bash
# First call
./.claude/hooks/on-user-prompt-tool-discovery.sh "multi-model analysis"
# Second call immediately (should be silent)
./.claude/hooks/on-user-prompt-tool-discovery.sh "multi-model analysis"
```

Expected: First call shows suggestion, second is silent

**Step 5: Commit**

```bash
git add .claude/hooks/on-user-prompt-tool-discovery.sh
git commit -m "feat: add UserPromptSubmit hook for real-time discovery

- Keyword-based detection for high-value tools
- Rate limiting (1 suggestion per 5 messages)
- 50ms timeout protection
- Logs errors without blocking prompt submission"
```

---

## Task 8: Create Log Directory

**Files:**
- Create: `.logs/` directory

**Step 1: Create log directory**

```bash
mkdir -p /home/ndninja/.logs
```

**Step 2: Test log writing**

```bash
echo "Tool discovery test log" >> /home/ndninja/.logs/tool-discovery.log
cat /home/ndninja/.logs/tool-discovery.log
```

Expected: Log file created and readable

**Step 3: Commit**

```bash
git add .logs/.gitkeep
git commit -m "feat: add logs directory for tool discovery

- Centralized location for hook error logs
- .gitkeep to preserve directory in git"
```

---

## Task 9: Integration Testing

**Files:**
- Create: `tests/test_integration.py`

**Step 1: Write integration test**

Create `tests/test_integration.py`:

```python
import pytest
import subprocess
from scripts.tool_discovery.database import ToolDatabase
from scripts.tool_discovery.session_start import SessionStartDiscovery
from scripts.tool_discovery.realtime import RealtimeDiscovery

def test_end_to_end_session_start():
    """Test complete SessionStart flow"""
    discovery = SessionStartDiscovery()
    tools = discovery.get_relevant_tools(limit=3)

    assert isinstance(tools, list)
    assert len(tools) <= 3

    output = discovery.format_suggestions(tools)
    assert 'Relevant tools' in output or 'No relevant tools' in output

def test_end_to_end_realtime():
    """Test complete real-time detection flow"""
    discovery = RealtimeDiscovery()

    # Test match
    prompt = "I need multi-model consensus analysis"
    tool = discovery.match_keywords(prompt)

    if tool:  # May be None if registry empty
        output = discovery.format_suggestion(tool)
        assert 'Relevant tool' in output

def test_hooks_executable():
    """Test that hooks are executable"""
    import os

    session_hook = '.claude/hooks/on-session-start-tool-discovery.sh'
    prompt_hook = '.claude/hooks/on-user-prompt-tool-discovery.sh'

    assert os.path.exists(session_hook)
    assert os.path.exists(prompt_hook)
    assert os.access(session_hook, os.X_OK)
    assert os.access(prompt_hook, os.X_OK)

def test_database_has_tools():
    """Test that registry is seeded"""
    db = ToolDatabase()
    db.connect()
    tools = db.get_all_tools()
    db.close()

    assert len(tools) >= 10  # Should have seeded tools
```

**Step 2: Run integration tests**

```bash
python3 -m pytest tests/test_integration.py -v
```

Expected: PASS (4/4 tests)

**Step 3: Commit**

```bash
git add tests/test_integration.py
git commit -m "test: add integration tests for tool discovery

- End-to-end SessionStart flow
- End-to-end real-time detection
- Hook executability checks
- Registry population verification"
```

---

## Task 10: Documentation

**Files:**
- Create: `scripts/tool_discovery/README.md`

**Step 1: Write README**

Create `scripts/tool_discovery/README.md`:

```markdown
# Memory-Assisted Tool Discovery

Proactive tool suggestion system that surfaces relevant custom tools based on conversation context.

## Components

### 1. SessionStart Discovery
- Runs on Claude Code session start
- Queries last 3 conversation summaries
- Matches topics against tool keywords
- Shows top 5 relevant tools

### 2. Real-time Detection
- Triggered on user prompt submission
- Regex matching for high-value keywords
- Rate limited (1 suggestion per 5 messages)
- Only suggests when highly relevant

### 3. Tool Registry
- Stored in claude-memory database
- Category: "tool", type: "reference"
- Contains metadata: name, description, keywords, usage, performance

## Usage

### Seed Registry
```bash
python3 /home/ndninja/scripts/tool_registry_seed.py
```

### View Registered Tools
```bash
psql -h localhost -U claude_mcp -d claude_memory -c \
  "SELECT key, title FROM references WHERE category='tool';"
```

### Test SessionStart Hook
```bash
./.claude/hooks/on-session-start-tool-discovery.sh
```

### Test Real-time Hook
```bash
./.claude/hooks/on-user-prompt-tool-discovery.sh "multi-model analysis"
```

## Configuration

### Disable Real-time Suggestions
```python
from scripts.tool_discovery.realtime import RealtimeDiscovery
# Edit RATE_LIMIT_MESSAGES = 999  # Effectively disabled
```

### Adjust Keywords
Edit `scripts/tool_discovery/realtime.py`:
```python
KEYWORD_PATTERNS = {
    r'your-pattern': 'tool-key',
    ...
}
```

### Add New Tool to Registry
```python
import psycopg2
import json

conn = psycopg2.connect(host='localhost', user='claude_mcp', database='claude_memory')
cursor = conn.cursor()

tool = {
    'name': 'My Tool',
    'description': '...',
    'keywords': ['...'],
    ...
}

cursor.execute("""
    INSERT INTO references (category, key, title, content, context_tag, device, created_at, updated_at)
    VALUES ('tool', %s, %s, %s, 'always', 'all', NOW(), NOW())
""", ('my-tool', tool['name'], json.dumps(tool)))

conn.commit()
```

## Troubleshooting

### No suggestions appearing
1. Check logs: `tail -f ~/.logs/tool-discovery.log`
2. Verify registry seeded: `python3 tool_registry_seed.py`
3. Test hooks manually (commands above)

### Rate limiting too aggressive
Edit `scripts/tool_discovery/realtime.py`:
```python
RATE_LIMIT_MESSAGES = 10  # Default: 5
```

### Hooks not running
Verify executability:
```bash
chmod +x .claude/hooks/on-*-tool-discovery.sh
```

## Architecture

```
User Input
    â†“
UserPromptSubmit Hook (realtime.py)
    â†“
Keyword Match? â†’ Tool Suggestion

Session Start
    â†“
SessionStart Hook (session_start.py)
    â†“
Query Recent Topics â†’ Rank Tools â†’ Top 5 Suggestions
```

## Testing

Run all tests:
```bash
python3 -m pytest tests/ -v
```

Run specific test suite:
```bash
python3 -m pytest tests/test_tool_discovery_db.py -v
python3 -m pytest tests/test_tool_scoring.py -v
python3 -m pytest tests/test_session_start.py -v
python3 -m pytest tests/test_realtime.py -v
python3 -m pytest tests/test_integration.py -v
```
```

**Step 2: Commit**

```bash
git add scripts/tool_discovery/README.md
git commit -m "docs: add README for tool discovery system

- Component overview
- Usage instructions
- Configuration guide
- Troubleshooting tips
- Architecture diagram
- Testing commands"
```

---

## Task 11: Update Skill Suggestions Table

**Files:**
- Database: `workspace.skill_suggestions`

**Step 1: Mark suggestion as implemented**

```bash
psql -h localhost -U claude_mcp -d workspace -c "
UPDATE skill_suggestions
SET status = 'implemented',
    implemented_at = NOW(),
    notes = 'Implemented as SessionStart/UserPromptSubmit hooks with shared Python module. ~285 lines of code. Estimated 1.5hr actual implementation time.'
WHERE title = 'Memory-Assisted Tool Discovery'
  AND status = 'pending';
"
```

**Step 2: Verify update**

```bash
psql -h localhost -U claude_mcp -d workspace -c "
SELECT title, status, implemented_at
FROM skill_suggestions
WHERE title = 'Memory-Assisted Tool Discovery';
"
```

Expected: Status = implemented, implemented_at = current timestamp

**Step 3: Commit**

```bash
git commit --allow-empty -m "chore: mark Memory-Assisted Tool Discovery as implemented

Updated skill_suggestions table:
- status: pending â†’ implemented
- implemented_at: $(date -Iseconds)
- Notes: Hook-based implementation, 285 LOC

Implementation complete and tested"
```

---

## Final Verification Checklist

- [ ] All tests passing: `python3 -m pytest tests/ -v`
- [ ] Registry seeded: 10 tools in database
- [ ] SessionStart hook working: Suggestions on new session
- [ ] Real-time hook working: Keyword detection functional
- [ ] Rate limiting verified: No spam
- [ ] Logs clean: No errors in `~/.logs/tool-discovery.log`
- [ ] Documentation complete: README.md written
- [ ] Git history clean: 11 commits with descriptive messages

## Success Metrics

After 1 week of usage:
- Tool discovery adds <300ms to session startup âœ“
- Real-time suggestions <50ms latency âœ“
- Zero session crashes âœ“
- User uses suggested tool at least 1x in 5 sessions (to measure)
- Suggestion relevance >70% (user feedback)

---

**Implementation Complete!**

Total estimated time: 1.5 hours
Actual tasks: 11 (bite-sized, 5-15 min each)
Total lines of code: ~285 (Python) + 50 (bash)
Test coverage: 16 tests across 5 test files
